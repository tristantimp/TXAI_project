{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 3/3 [00:00<00:00, 87.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ik hou van programmeren.\n",
      "{'input_ids': tensor([[250004,   3256,  19660,    131,  77848,     33,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Het regent vandaag.\n",
      "{'input_ids': tensor([[250004,   1947, 119555,     18,  64697,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "De kat zit op de mat.\n",
      "{'input_ids': tensor([[250004,    262,   3133,  21583,    233,      8,   2589,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 283.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De overheid heeft vandaag aangekondigd dat de belastingen volgend jaar zullen stijgen vanwege economische omstandigheden.\n",
      "{'input_ids': tensor([[250004,    262, 118541,   3188,  64697, 233033,     71,    607,      8,\n",
      "         110119,     33,  73172,     71,   3325,  33607,  13629,    170,   1409,\n",
      "         142726, 221584, 210492,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Na een spannende wedstrijd won Ajax met 2-1 van PSV in de laatste minuut.\n",
      "{'input_ids': tensor([[250004,    353,    293, 124238,  78336,  23742, 157715,    435,  58671,\n",
      "            131,   7940,    856,     23,      8,  31836, 141259,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Wetenschappers ontdekten een nieuwe planeet buiten ons zonnestelsel.\n",
      "{'input_ids': tensor([[250004,   1401,    510,  28624,  21777,  98178,    510,    293,   9800,\n",
      "         157402,  28605,   1260, 148906,  96638,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 275.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wat is de hoofdstad van Nederland? Amsterdam is de grootste stad van Nederland en ook de hoofdstad.\n",
      "{'input_ids': tensor([[250004,   6586,     83,      8,  35912,   5481,    131,  13666,     32,\n",
      "          21391,     83,      8,  53653,  18961,    131,  13666,     22,   1232,\n",
      "              8,  35912,   5481,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Wie schreef het boek 'De Avonden'? Gerard Reve schreef het in 1947.\n",
      "{'input_ids': tensor([[250004,   4887, 116648,    225,  14666,    242,   4657, 147202,    555,\n",
      "             25,     32, 125947, 110230, 116648,    225,     23,  40191,      5,\n",
      "              2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Wat is de langste rivier in Nederland? De Rijn stroomt door meerdere landen en is de langste rivier.\n",
      "{'input_ids': tensor([[250004,   6586,     83,      8,   1937,    824, 141808,     23,  13666,\n",
      "             32,    262,    627,  25536,  26678,    306,     18,   1911,  77864,\n",
      "          87294,     22,     83,      8,   1937,    824, 141808,      5,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"MT\": [\n",
    "        (\"Ik hou van programmeren.\", \"I love programming.\"),\n",
    "        (\"Het regent vandaag.\", \"It is raining today.\"),\n",
    "        (\"De kat zit op de mat.\", \"The cat is sitting on the mat.\")\n",
    "    ],\n",
    "    \"TS\": [\n",
    "        (\"De overheid heeft vandaag aangekondigd dat de belastingen volgend jaar zullen stijgen vanwege economische omstandigheden.\", \"Belastingen stijgen volgend jaar.\"),\n",
    "        (\"Na een spannende wedstrijd won Ajax met 2-1 van PSV in de laatste minuut.\", \"Ajax wint met 2-1 van PSV.\"),\n",
    "        (\"Wetenschappers ontdekten een nieuwe planeet buiten ons zonnestelsel.\", \"Nieuwe planeet ontdekt.\")\n",
    "    ],\n",
    "    \"QA\": [\n",
    "        (\"Wat is de hoofdstad van Nederland? Amsterdam is de grootste stad van Nederland en ook de hoofdstad.\", \"Amsterdam\"),\n",
    "        (\"Wie schreef het boek 'De Avonden'? Gerard Reve schreef het in 1947.\", \"Gerard Reve\"),\n",
    "        (\"Wat is de langste rivier in Nederland? De Rijn stroomt door meerdere landen en is de langste rivier.\", \"De Rijn\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "for task_name, samples in datasets.items():\n",
    "    for input_text, reference in tqdm(samples):\n",
    "        print(input_text)\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trist_243cr5c\\Documents\\block 3\\TXAI\\lm-polygraph\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from lm_polygraph.estimators import *\n",
    "from lm_polygraph.utils.model import WhiteboxModel\n",
    "from lm_polygraph.utils.dataset import Dataset\n",
    "from lm_polygraph.utils.processor import Logger\n",
    "from lm_polygraph.utils.manager import UEManager\n",
    "from lm_polygraph.ue_metrics import PredictionRejectionArea\n",
    "from lm_polygraph.generation_metrics import RougeMetric, BartScoreSeqMetric, ModelScoreSeqMetric, ModelScoreTokenwiseMetric, AggregatedMetric\n",
    "from lm_polygraph.utils.builder_enviroment_stat_calculator import (\n",
    "    BuilderEnvironmentStatCalculator\n",
    ")\n",
    "from lm_polygraph.defaults.register_default_stat_calculators import (\n",
    "    register_default_stat_calculators,\n",
    ")\n",
    "from lm_polygraph.utils.factory_stat_calculator import StatCalculatorContainer\n",
    "from omegaconf import OmegaConf\n",
    "from lm_polygraph import estimate_uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('bigscience/bloomz-560m', device_map='cpu',)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/bloomz-560m')\n",
    "model = WhiteboxModel(base_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UncertaintyOutput(uncertainty=array([-0.16278946, -0.40806034, -0.38379776], dtype=float32), input_text='Qui est George Bush?', generation_text=' le président américain', generation_tokens=[578, 17635, 52762], model_path=None, estimator='MaximumTokenProbability')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MaximumTokenProbability()\n",
    "estimate_uncertainty(model, estimator, input_text='Qui est George Bush?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.7690080404281616],\n",
       " 'recall': [0.7674813270568848],\n",
       " 'f1': [0.7682439088821411],\n",
       " 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.50.1)'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"de kat slaapt op de bank\"]\n",
    "references = [\"een poes ligt op een sofa\"]\n",
    "bertscore.compute(predictions=predictions, references=references, lang=\"nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_type = \"Whitebox\"\n",
    "dataset_name = (\"trivia_qa\", \"rc.nocontext\")\n",
    "batch_size = 4\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load(\n",
    "    dataset_name,\n",
    "    'question', 'answer',\n",
    "    batch_size=batch_size,\n",
    "    prompt=\"Question: {question}\\nAnswer:{answer}\",\n",
    "    split=\"validation\"\n",
    ")\n",
    "dataset.subsample(1, seed=seed)\n",
    "\n",
    "train_dataset = Dataset.load(\n",
    "    dataset_name,\n",
    "    'question', 'answer',\n",
    "    batch_size=batch_size,\n",
    "    prompt=\"Question: {question}\\nAnswer:{answer}\",\n",
    "    split=\"train\"\n",
    ")\n",
    "train_dataset.subsample(1, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_methods = [MaximumSequenceProbability(), \n",
    "              SemanticEntropy(),\n",
    "              MahalanobisDistanceSeq(\"decoder\"),\n",
    "             ]\n",
    "\n",
    "ue_metrics = [PredictionRejectionArea(), PredictionRejectionArea(max_rejection=0.5)]\n",
    "\n",
    "# Wrap generation metric in AggregatedMetric, since trivia_qa is a multi-reference dataset\n",
    "# (y is a list of possible correct answers)\n",
    "metrics = [AggregatedMetric(RougeMetric('rougeL'))]\n",
    "\n",
    "loggers = [Logger()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingStatistic_config = {\n",
    "    \"dataset\": dataset_name,\n",
    "    \"text_column\": 'question',\n",
    "    \"label_column\": 'answer',\n",
    "    \"description\": '',\n",
    "    \"prompt\": \"Question: {question}\\nAnswer:\",\n",
    "    \"few_shot_split\": 'train',\n",
    "    \"train_split\": 'train',\n",
    "    \"load_from_disk\": False,\n",
    "    \"subsample_train_dataset\": 10,\n",
    "    \"n_shot\": 5,\n",
    "    \"train_dataset\": dataset_name,\n",
    "    \"train_test_split\": False,\n",
    "    \"background_train_dataset\": 'allenai/c4',\n",
    "    \"background_train_dataset_text_column\": 'text',\n",
    "    \"background_train_dataset_label_column\": 'url',\n",
    "    \"background_train_dataset_data_files\": 'en/c4-train.00000-of-01024.json.gz',\n",
    "    \"background_load_from_disk\": False,\n",
    "    \"subsample_background_train_dataset\": 10,\n",
    "    \"batch_size\": 1,\n",
    "    \"seed\": 1,\n",
    "    \"size\": 1,\n",
    "    \"bg_size\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_stat_calculators = dict()\n",
    "scs = register_default_stat_calculators(model_type)\n",
    "for sc in scs:\n",
    "    result_stat_calculators[sc.name] = sc\n",
    "\n",
    "# register TrainingStatisticExtractionCalculator for the Mahalanobis Distance method\n",
    "result_stat_calculators.update(\n",
    "    {\n",
    "        \"TrainingStatisticExtractionCalculator\": StatCalculatorContainer(\n",
    "            name=\"TrainingStatisticExtractionCalculator\",\n",
    "            cfg=OmegaConf.create(TrainingStatistic_config),\n",
    "            stats=[\"train_embeddings\", \"background_train_embeddings\", \"train_greedy_log_likelihoods\"],\n",
    "            dependencies=[],\n",
    "            builder=\"lm_polygraph.defaults.stat_calculator_builders.default_TrainingStatisticExtractionCalculator\",\n",
    "        )\n",
    "    }\n",
    ")\n",
    "    \n",
    "builder_env_stat_calc = BuilderEnvironmentStatCalculator(model=model)\n",
    "available_stat_calculators = list(result_stat_calculators.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating train split: 356317 examples [00:06, 59020.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "man = UEManager(\n",
    "    data=dataset,\n",
    "    model=model,\n",
    "    estimators=ue_methods,\n",
    "    builder_env_stat_calc=builder_env_stat_calc,\n",
    "    available_stat_calculators=available_stat_calculators,\n",
    "    generation_metrics=metrics,\n",
    "    ue_metrics=ue_metrics,\n",
    "    processors=loggers,\n",
    "    ignore_exceptions=False,\n",
    "    max_new_tokens=10, \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n",
      "c:\\Users\\trist_243cr5c\\Documents\\block 3\\TXAI\\lm-polygraph\\.env\\Lib\\site-packages\\lm_polygraph\\estimators\\mahalanobis_distance.py:35: UserWarning: cov(): degrees of freedom is <= 0. Correction should be strictly less than the number of observations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Correlation.cpp:116.)\n",
      "  cov_scaled = torch.cov(train_features.T)\n",
      "100%|██████████| 1/1 [00:17<00:00, 17.14s/it]\n",
      "WARNING:lm_polygraph:We got 1 nans in MahalanobisDistanceSeq_decoder estimator.\n",
      "c:\\Users\\trist_243cr5c\\Documents\\block 3\\TXAI\\lm-polygraph\\.env\\Lib\\site-packages\\lm_polygraph\\ue_metrics\\pred_rej_area.py:52: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prr_score = np.sum(scores) / num_rej\n",
      "WARNING:lm_polygraph:We got 1 nans in MahalanobisDistanceSeq_decoder estimator.\n",
      "  0%|          | 0/1 [00:17<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "results = man()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr, Score: 0.500\n",
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr_normalized, Score: 0.500\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr, Score: 0.500\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr_normalized, Score: 0.500\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr, Score: 0.500\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr_normalized, Score: 0.500\n",
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr_0.5, Score: nan\n",
      "UE Score: MaximumSequenceProbability, Metric: Rouge_rougeL, UE Metric: prr_0.5_normalized, Score: nan\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr_0.5, Score: nan\n",
      "UE Score: SemanticEntropy, Metric: Rouge_rougeL, UE Metric: prr_0.5_normalized, Score: nan\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr_0.5, Score: nan\n",
      "UE Score: MahalanobisDistanceSeq_decoder, Metric: Rouge_rougeL, UE Metric: prr_0.5_normalized, Score: nan\n"
     ]
    }
   ],
   "source": [
    "for key in results.keys():\n",
    "    print(f\"UE Score: {key[1]}, Metric: {key[2]}, UE Metric: {key[3]}, Score: {results[key]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
